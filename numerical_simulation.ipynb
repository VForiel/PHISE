{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# üìú **Abstract**\n",
    "\n",
    "</div>\n",
    "\n",
    "Nulling interferometry $^1$ is a promising technique for direct detection of exoplanets. However, the performance of current devices is limited by the sensitivity to any phase aberrations. The work presented here attempts to overcome those limitations by using a four-telescopes nulling interferometer architecture, called Kernel-Nuller $^2$, which includes a recombiner that positions the four signals in phase quadrature. This architecture is based on an integrated optical component containing 14 electronically controlled phase shifters, used to correct optical path differences that would be induced by manufacturing defects. The first part of the study consists in the development of an algorithm providing the delays to be injected into the component to optimize the performance of that device. The next step of this study deals with the analysis of the intensity distributions produced at the output of the Kernel-Nuller $^{2,3}$ through a series of observations, and then apply statistical tests and data treatment techniques to detect the presence of exoplanets.\n",
    "\n",
    "> **References**\n",
    "> 1. Bracewell, R.N., MacPhie, R.H., 1979. Searching for nonsolar planets. Icarus 38, 136‚Äì147. https://doi.org/10.1016/0019-1035(79)90093-9\n",
    "> 2. Martinache, F., Ireland, M.J., 2018. Kernel-nulling for a robust direct interferometric detection of extrasolar planets. A&A 619, A87. https://doi.org/10.1051/0004-6361/201832847\n",
    "> 3. Cvetojevic, N. et al., 2022. 3-beam self-calibrated Kernel nulling photonic interferometer. arXiv e-prints. https://doi.org/10.48550/arXiv.2206.04977\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ‚ùó**Assumptions**\n",
    "\n",
    "</div>\n",
    "\n",
    "+ The banwidth $ŒîŒª$ is small enough to consider that:\n",
    "  - The flux is constant for all wavelengths $\\rightarrow F_Œª = F$\n",
    "  - The refractive index of the material is constant over the bandwidth $\\rightarrow n_Œª = n$ so phase shifters can be considered as pistons. We then express the phase shifts as OPDs (in distance unit).\n",
    "+ The star is not resolved so it can be associated to a point source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ü§î **Context**\n",
    "\n",
    "</div>\n",
    "\n",
    "## üéØ Goal\n",
    "\n",
    "We aim to detect make direct detection of exoplanets. There is two main challenges to achieve this goal:\n",
    "- The **contrast**: the exoplanet is much fainter than the star it orbits. The contrast is typically of the order of $10^{-6}$ to $10^{-10}$.\n",
    "- The **angular separation**: the exoplanet is very close to the star. The angular separation depend on the distance of the exoplanet to the star and the distance of the star to the observer and can easily goes below the arcsecond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Detection methods\n",
    "\n",
    "It exist several methods to detect exoplanets. The most common are:\n",
    "- **Radial velocity method**: the exoplanet induce a wobble in the star it orbits. This wobble can be detected by the Doppler effect (the light is alternatively redshifted and blueshifted).\n",
    "- **Transit method**: the exoplanet pass in front of the star and block a fraction of the light. This fraction can be detected by the decrease of the star luminosity.\n",
    "- **Microlensing**: the exoplanet act as a lens and magnify the light of a background star. This magnification can be detected by the increase of the star luminosity.\n",
    "- **Astrometry**: the exoplanet induce a wobble in the star it orbits. This wobble can be detected by the change of the star position.\n",
    "- **Coronography**: the exoplanet is directly imaged. This is the most challenging method because of the contrast and the angular separation.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"img/detection_methods.jpg\" width=500px>\n",
    "<p><i>Paul Anthony Wilson - Exoplanet detection techniques</i><p>\n",
    "</div>\n",
    "\n",
    "Until now, the coronography was the only method allowing direct detection. But it has two main limitations:\n",
    "- It require huge telescopes in order to have a good angular resolution.\n",
    "- The contrast we can achieve is limited by unperfect fabrication process of the optical components which lead to undesired diffraction effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ûñ Nulling\n",
    "\n",
    "This is where the Nulling technic $^1$ come into play. The idea is to use two several telescopes and take advantage of destructives interferances to cancel the star light and using the fact that the planet is not perfectly in the line of sight, which will lead to an unperfect destructive interference, or in best scenarios, on constructive ones! This is a very challenging technic because it is highly phase sensitive and require a very good control of the optical path.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"img/nulling_principle.jpg\" width=500px>\n",
    "</div>\n",
    "\n",
    "In a perfect 2-telescope nulling system, we can express the two acquired electric field (cf. section \"Signal nature\" below) in a vector:\n",
    "\n",
    "$$\n",
    "E = \\begin{pmatrix}\n",
    "E_1 \\\\\n",
    "E_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And then express the nulling operation using the following matrix:\n",
    "\n",
    "$$\n",
    "N = \\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Such as it gives us two outputs with a constructive and a destructive interference (we will focus on the latter):\n",
    "\n",
    "$$\n",
    "N \\cdot E = \\begin{pmatrix}\n",
    "E_1 + E_2 \\\\\n",
    "E_1 - E_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "> **References**\n",
    "> 1. Bracewell, R.N., MacPhie, R.H., 1979. Searching for nonsolar planets. Icarus 38, 136‚Äì147. https://doi.org/10.1016/0019-1035(79)90093-9\n",
    "\n",
    "cf. `mmi.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì° Projected telescope position\n",
    "\n",
    "The interferometry process depend on the projected geometry of the telescope position in the plane perpendicular to the line of sight. For each observation, we will then need to compute these projected positions in order to have the correct baseline lenght (and thus the correct phase shifts).\\\n",
    "These projected location can be computed using the following formula $^{1,2}$:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "u \\\\\n",
    "v\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "- \\sin(l) \\sin(h) & \\cos(h)\\\\\n",
    "\\sin(l) \\cos(h) \\sin(\\delta) + \\cos(l) \\cos(\\delta) & \\sin(h) \\sin(\\delta)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "B_\\text{north} \\\\\n",
    "B_\\text{east}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with\n",
    "- $l$ the latitude of the observatory\n",
    "- $h$ the hour angle\n",
    "- $\\delta$ the declination of the star\n",
    "\n",
    "> *Rerefence*\n",
    "> 1. Chingaipe, P.M. et al., 2023. High-contrast detection of exoplanets with a kernel-nuller at the VLTI. A&A 676, A43. https://doi.org/10.1051/0004-6361/202346118\n",
    "> 2. S√©gransan, D., 2007. Observability and UV coverage. New Astronomy Reviews 51, 597‚Äì603. https://doi.org/10.1016/j.newar.2007.06.005\n",
    "\n",
    "cf. `project_position_njit()` in `src.classes.context.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "analysis.projected_telescopes.gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÜ Signal nature\n",
    "\n",
    "The star and the planet are point sources. Seen from a classical telescope, it will result in an image made of the objects convolution with the point spread function (PSF) of the telescope.\n",
    "\n",
    "$$\n",
    "I = O \\otimes PSF\n",
    "$$\n",
    "\n",
    "Here we consider the most simple PSF : the Airy disk. The Airy disk is the diffraction pattern of a point source by a circular aperture. It is given by:\n",
    "\n",
    "$$\n",
    "PSF = \\left(\\frac{2J_1(x)}{x}\\right)^2\n",
    "$$\n",
    "\n",
    "where $J_1$ is the Bessel function of the first kind of order 1 and $x = \\frac{2\\pi r}{\\lambda f}$ is the normalized radius of the Airy disk.\n",
    "\n",
    "Then, we focus the image in a monomode optical fiber which will basically only keep the main lobe of the PSF and reshape it in a Gaussian form. In this process, we lose the spatial information so we have no longer images, but the light flux of each object in the fiber can be simply described by a complex number.\n",
    "\n",
    "Using this formalism, the light flux of the star and the planet can  be described by only 2 complex numbers for each telescope, giving the amplitude and phase of each object.\n",
    "\n",
    "cf. `signals.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîâ Photon noise\n",
    "\n",
    "The photon noise is the noise due to the quantization of the light in photons. It is a Poisson noise and can be described by the following formula:\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{N}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of photons.\n",
    "\n",
    "The number of photons can be computed using the Light flux $F_\\lambda$ of the star and the planet, the collecting area $A$ of the telescope, the bandwidth $\\Delta \\lambda$, the transmission efficiency $\\eta$, the magnitude $M$ of the star, and the energy of a photon $E_\\nu$ at the frequency $\\nu$.\n",
    "\n",
    "Light flux is expressed in Jensky:\n",
    "$$\n",
    "1\\:\\text{Jensky} = 1\\:Jy = 10^{-26}\\:W/m^2/Hz = 10^{-26}\\:J/s/m^2/Hz\n",
    "$$\n",
    "The flux at zero magnitude $F_\\lambda$ for a star at different wavelengths is given by tables. The following one is an example for AB class stars such as Vega $^1$:\n",
    "\n",
    "| Band | $\\lambda$ ($\\mu m$) | $F_\\lambda$ ($Jy$) |\n",
    "|------|---------------------|--------------------|\n",
    "| V    | 0.55                | 3540               |\n",
    "| J    | 1.21                | 1630               |\n",
    "| H    | 1.65                | 1050               |\n",
    "| K    | 2.17                | 655                |\n",
    "| L    | 3.55                | 276                |\n",
    "\n",
    "The number of acquired photons per second is given by:\n",
    "\n",
    "$$\n",
    "N = F_\\lambda \\times A \\times \\Delta \\nu \\times \\eta \\times 10^{-\\frac{M}{2.5}} / E_\\nu\n",
    "$$\n",
    "\n",
    "where $A$ is the collecting area of the telescope, $\\Delta \\nu$ is the bandwidth, $\\eta$ is the transmission efficiency of the system, $M$ is the magnitude of the star, and $E_\\nu$ is the energy of a photon at the frequency $\\nu$.\n",
    "\n",
    "We can also express the bandwidth and eneergy in terms of the wavelength:\n",
    "$$\n",
    "\\Delta \\nu = \\frac{c}{\\lambda^2} \\Delta \\lambda\n",
    "$$\n",
    "$$\n",
    "E_\\nu = \\frac{h \\times c}{\\lambda}\n",
    "$$\n",
    "\n",
    "where $c$ is the speed of light, $h$ is the Planck constant.\n",
    "\n",
    "> **References**\n",
    "> 1. Allen's Astrophysical Quantities\n",
    "\n",
    "cf. `signals.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÆ MMI\n",
    "\n",
    "The nulling operation is made using Multi Mode Interferometer (MMI). It consist in a multimode waveguide taking several monomode fibers as input and output. The multimode waveguide is shaped in order to produce a specific interference operation, such as spreading the light of an input on all the output, or opposite, gathering the light of all the input on a single output.\n",
    "\n",
    "To design a simple nuller, we then need a 2x2 MMI that gather (ie. create a constructive interference) all the input light on a single output. The other output is then a \"nulled\" output, where there is actually all the inputs light but in phase opposition, resulting in a destructive interference.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"img/mmi.png\" width=400px>\n",
    "\n",
    "*Numerical simulation of a 3x3 gathering MMI, taken from the paper of Cvetojevic et. al., 2022 $^1$*\n",
    "\n",
    "</div>\n",
    "\n",
    "> **Reference**\n",
    "> 1. Cvetojevic, N. et al., 2022. 3-beam self-calibrated Kernel nulling photonic interferometer. arXiv e-prints. https://doi.org/10.48550/arXiv.2206.04977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Recombiner\n",
    "\n",
    "The recombiner is also an MMI that will place the signals in phase quadrature. A particularity is that the output of the recombiner contain a symmetry. We will take advantage of this in the Kernel step.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"img/recombiner.png\" width=500px>\n",
    "\n",
    "*Action of a 2x2 recombiner MMI, taking 2 different combination of 4 nulled signals as input. Taken from the paper of Romain Laugier et al., 2020 $^1$*\n",
    "\n",
    "</div>\n",
    "\n",
    "In a 2 input case, we can express the recombiner operation using the following matrix:\n",
    "\n",
    "$$  \n",
    "R = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\n",
    "e^{i\\pi/4} & e^{-i\\pi/4} \\\\\n",
    "e^{-i\\pi/4} & e^{i\\pi/4}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "> **Reference**\n",
    "> 1. Laugier, R., Cvetojevic, N., Martinache, F., 2020. Kernel nullers for an arbitrary number of apertures. A&A 642, A202. https://doi.org/10.1051/0004-6361/202038866\n",
    "\n",
    "cf. `mmi.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí† Kernel\n",
    "\n",
    "The idea of the kernel is to acquire and substract the pairs of recombined output. As these pairs share symmetrical properties, this substraction will cancel the star light even with first order phase aberations while keeping the planet light!\n",
    "\n",
    "Moreover, it modify the nuller response (cf. \"Transmission maps\" section below) in an asymetric way which is interesting for us as it gives us more information to constrain the planet position.\n",
    "\n",
    "Demonstration (cf. `demonsration.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "analysis.demonstration.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå°Ô∏è Thermo-optic phase shifter\n",
    "\n",
    "In practice, we are often limited by the fabrication process of the optical components. The imperfections can lead into phase aberations that will degrade the Kernel-Nuller performance. An attempt to correct these aberations consist in using thermo-optic phase shifters. It consist in a waveguide with a heater that will modify the refractive index of the waveguide and thus the phase of the light passing through it.\n",
    "\n",
    "As the size of the waveguide is very small, the thermal inertia is very low and the phase can be modified very quickly, in a milisecond time scale. This is a very interesting solution to correct phase aberations, even in real time if we encounter variable phase aberation sources.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"img/thermo-optic_phase_shifter.png\" width=500px>\n",
    "</div>\n",
    "\n",
    "In this simulation, one will simply modelize these phase shifter as a phase shift in the signal.\n",
    "\n",
    "$$\n",
    "P = e^{i\\phi} = e^{i\\frac{2\\pi}{\\lambda} \\Delta L}\n",
    "$$\n",
    "\n",
    "Where $\\phi$ and $\\Delta L$ are proportional to the power injected in the heater.\n",
    "\n",
    "cf. `phase.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# üí° **Our approach**\n",
    "\n",
    "</div>\n",
    "\n",
    "## üèóÔ∏è Current architecture\n",
    "\n",
    "To implement the 4 telescope tunable Kernel-Nuller, we splitted the 4x4 MMI into series of 2x2 MMI separated by phase shifters.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"img/scheme.png\" width=1000px>\n",
    "\n",
    "*Architecture of our Kernel-Nuller. N squares are the 2x2 nullers, S squares are the 2x2 recombiners and P rectangles are the phase shifters*\n",
    "\n",
    "</div>\n",
    "\n",
    "cf. `kn.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Transmission maps\n",
    "\n",
    "The nulling technic with two telescope show a limitation: if the planet light arrive on the two telescopes with a phase shift of $2n\\pi$, the light will also be cancelled. It result in a comb-shaped transmission map $^1$, perpendicular to the baseline (there is clear bands where it's optimal to detect the planet and black bands where we will mostly destroy the planet light).\n",
    "\n",
    "The idea of Bracewell was to rotate the baseline in order to let the planet pass through the clear bands at some point. After an entire rotation of the baseline, we will have a sinusoidal signal from which the frequency will indicate us the distance of the planet to it's star, and the phase will give us a clue about the angle between the axes star-planet and the axes of the baseline. Thus, as the transmission map is symmetric, we can constrain the planet position to 2 possible locations, on both sides of the star.\n",
    "\n",
    "Here, we are using 4 telescopes, resulting in more complexe transmission maps than simple combs, but the principle is the same.\n",
    "\n",
    "> **Reference**\n",
    "> 1. Bracewell, R.N., MacPhie, R.H., 1979. Searching for nonsolar planets. Icarus 38, 136‚Äì147. https://doi.org/10.1016/0019-1035(79)90093-9\n",
    "\n",
    "cf. `transmission_map.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "analysis.transmission_maps.gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ü™õ **Calibration**\n",
    "\n",
    "</div>\n",
    "\n",
    "## ü´≥ Manual shift controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "analysis.manual_control.gui(Œª=1.65 * u.um, œÜ=np.zeros(14) * u.nm, œÉ=np.abs(np.random.normal(0,100, 14)) * u.nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Genetic\n",
    "\n",
    "in order to get the best shifts to inject to optimize the component performances, I made a genetic algorithme that iteratively mutate a shifter and keep the mutation if it minimize the associated metric. All the shifters that act on the bright channel are associated with the bright metric that must be maximized\n",
    "\n",
    "$$\n",
    "M_B = |B|^2\n",
    "$$\n",
    "\n",
    "While the other shifters are associated with the kernel metric that must be minimized.\n",
    "\n",
    "$$\n",
    "M_K = \\sum_{n=1}^3|K_n|\n",
    "$$\n",
    "\n",
    "Merging these two metrics can induce local minimum since improving the bright metric can deterior the kernel metric. This separation is then necessary to ensure reaching a global minimum (empirically demonstrated)\n",
    "\n",
    "> **Acknowledgment**\n",
    "> - Romain Laugier for the idea of merging the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "_ = analysis.calibration.genetic_approach(Œ≤=0.9, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÅÔ∏è Obstruction\n",
    "\n",
    "En obstruant successivement une paire d'entr√©es, il est possible de simplifier le probl√®me d'optimisation en ne jouant que sur un seul param√®tre et en ne regardant qu'une seule sortie.\n",
    "\n",
    "Il existe diff√©rentes fa√ßon de proc√©der. Je ne vais d√©tailler ici que l'une d'entre elles.\n",
    "\n",
    "On commence par obstruer les entr√©es $I_2$ et $I_3$. Au regard de l'architectue de notre composant, on peut alors d√©crire la fonction de transfert pour la sortie brillante $B$\n",
    "\n",
    "$$\n",
    "B = \\left|\\left(a_1 e^{i(\\theta_1 + \\sigma_1 + \\phi_1)} + a_2 e^{i(\\theta_2 + \\sigma_2 + \\phi_2)}\\right) e^{i(\\sigma_5 + \\phi_5)}\\right|^2\n",
    "$$\n",
    "\n",
    "O√π $a_n$ et $\\theta_n$ repr√©sentent respectivement l'amplitude et la phase des signaux d'entr√©e. $\\sigma_n$ correspond √† la perturbation de phase (inconnue) associ√© au retardateur $n$ et $\\phi_n$ est la phase que l'on inject volontairement via le retardateur pour tenter de compenser cette perturbation.\n",
    "\n",
    "La calibration se faisant en laboatoire, on peut supposer une intensit√© totale fix√©e √† $1$ (unit√© arbitraire) et que chaque entr√©e rec√ßoi le m√™me flux soit $a_1 = a_2 = 1/\\sqrt{2}$, et parfaitement cophas√©, soit $\\theta_1 = \\theta_2 = \\theta$. Etant donn√© que l'on a acc√®s qu'a l'intensit√© du signal, nous sommes insensible √† la phase globale, ce qui permet de simplifier l'√©quation pr√©c√©dente :\n",
    "\n",
    "$$\n",
    "B = \\frac{1}{2} \\left|e^{i(\\sigma_1 + \\phi_1)} + e^{i(\\sigma_2 + \\phi_2)}\\right|^2\n",
    "$$\n",
    "\n",
    "En maximisant $B$, on devrait alors trouver $1$ ce qui implique que\n",
    "\n",
    "$$\n",
    "\\sigma_1 + \\phi_1 = \\sigma_2 + \\phi_2\n",
    "$$\n",
    "\n",
    "On peut utiliser $\\phi_1$ comme r√©f√©rence (phase globale) et ainsi le fixer √† 0, ce qui donne alors\n",
    "\n",
    "$$\n",
    "\\phi_2 = \\sigma_1 - \\sigma_2\n",
    "$$\n",
    "\n",
    "On peut alors soit effectuer diff√©rentes mesures de $B$ √† $\\phi_2$ fix√© et en d√©duire $\\sigma_1$ et $\\sigma_2$ par r√©solution d'un syst√®me d√©quation, soit trouver dichotomiquement la valeur de $\\phi_2$ qui maximise $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "_ = analysis.calibration.obstruction_approach(N=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßê Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "analysis.calibration.compare_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Machine Learning\n",
    "\n",
    "Another approach to retrieve the correct shifts inject is to use machine learning techniques. There is several ways to do so. Here we will focus a supervised dense neural network. To do so, we will have to build a dataset.\n",
    "\n",
    "As the solutions are degenerated, we will not ask the network to predict the best shift to inject, but we will ask it to predict the shfit aberrations instead. From these aberation, we are able to determine a solution for the shifts to inject.\n",
    "\n",
    "As input of the network, we need to give enough information to caracterize the parameter space. The most straightforward approach would be to create a grid in the parameter space and give the kenrel outputs for each of the point in these grid.\n",
    "\n",
    "Unfortunately, we work in a parameter space of 14 dimensions which is too large to be covered by a grid. A solution is to consider only the vectors that form the cardinal basis of this parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor the following code\n",
    "# DATASET = ml.get_dataset(10_000)\n",
    "# print(DATASET.shape)\n",
    "# MODEL = ml.get_model(input_shape=DATASET.shape[1]-14)\n",
    "# MODEL.summary()\n",
    "# ml.train_model(plot=True)\n",
    "# ml.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ‚öôÔ∏è **Data generation**\n",
    "\n",
    "</div>\n",
    "\n",
    "First, let's generate the different scenes we will compare:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü® Instantaneous serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_serie.plot_instant_serie(\n",
    "    scenes = ALL_SCENES,\n",
    "    contrast = 1e-6,\n",
    "    input_ce_rms = INPUT_CE_RMS,\n",
    "    Œ± = 0 * u.deg,\n",
    "    Œ∏ = 2 * u.mas,\n",
    "    figsize=(15, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü° Time serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_VACUUM_IDEAL = SCENE_IDEAL.copy(input_ce_rms=0*u.nm)\n",
    "\n",
    "H1_VACUUM_IDEAL_TIME_DATA = ObservationSerie(\n",
    "    scene=SCENE_VACUUM_IDEAL.copy()\n",
    ")\n",
    "\n",
    "H1_IDEAL_TIME_DATA = ObservationSerie(\n",
    "    scene=SCENE_IDEAL.copy()\n",
    ")\n",
    "\n",
    "\n",
    "H1_OBS_TIME_DATA = ObservationSerie(\n",
    "    scene=SCENE_OBS.copy()\n",
    ")\n",
    "\n",
    "H1_GEN_TIME_DATA = ObservationSerie(\n",
    "    scene=SCENE_GEN.copy()\n",
    ")\n",
    "\n",
    "H1_PERTURBED_TIME_DATA = ObservationSerie(\n",
    "    scene=SCENE_PERTURBED.copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_serie.plot_time_serie(\n",
    "    H1_VACUUM_IDEAL_TIME_DATA,\n",
    "    H1_OBS_TIME_DATA,\n",
    "    H1_GEN_TIME_DATA,\n",
    "    figsize=(20,5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_time_serie(window):\n",
    "\n",
    "    nb_points_in_window = int(window.to(u.hour).value / ŒîT.to(u.hour).value)\n",
    "    nb_widows = int(ŒîH.to(u.hourangle).value / window.to(u.hour).value)\n",
    "\n",
    "    mx = 0\n",
    "    mn = 0\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20,5))\n",
    "    for k in range(3):\n",
    "        axs[k].set_title(f'Kernel {k+1}')\n",
    "        for night in range(VACUUM_IDEAL_TIME_SERIE['kernels'].shape[0]):\n",
    "\n",
    "            # Median\n",
    "            h_range_medians = np.zeros(nb_points_in_window) * H_RANGE.unit\n",
    "            gen_medians = np.zeros(nb_points_in_window)\n",
    "            obs_medians = np.zeros(nb_points_in_window)\n",
    "\n",
    "            for i in range(nb_points_in_window):\n",
    "                h_range_medians[i] = np.median(H_RANGE[i*nb_points_in_window:(i+1)*nb_points_in_window])\n",
    "                gen_medians[i] = np.median(GEN_TIME_SERIE['kernels'][night,i*nb_points_in_window:(i+1)*nb_points_in_window,k])\n",
    "                obs_medians[i] = np.median(OBS_TIME_SERIE['kernels'][night,i*nb_points_in_window:(i+1)*nb_points_in_window,k])\n",
    "\n",
    "            axs[k].plot(h_range_medians, gen_medians, marker='.', color='tab:orange', linewidth=1, alpha=1, label='Gen. calibration')\n",
    "            axs[k].plot(h_range_medians, obs_medians, marker='.', color='tab:blue', linewidth=1, alpha=1, label='Obs. calibration')\n",
    "            axs[k].scatter(H_RANGE, VACUUM_IDEAL_TIME_SERIE['kernels'][night,:,k], label='Ideal', color='k', s=1)\n",
    "\n",
    "            mx = max(\n",
    "                mx,\n",
    "                np.max(gen_medians),\n",
    "                np.max(obs_medians),\n",
    "                np.max(VACUUM_IDEAL_TIME_SERIE['kernels'])\n",
    "            )\n",
    "\n",
    "            mn = min(\n",
    "                mn,\n",
    "                np.min(gen_medians),\n",
    "                np.min(obs_medians),\n",
    "                np.min(VACUUM_IDEAL_TIME_SERIE['kernels'])\n",
    "            )\n",
    "\n",
    "        axs[k].set_ylim(mn, mx)\n",
    "        axs[k].set_xlabel('Hour angle [h]')\n",
    "        axs[k].set_ylabel('Kernel value')\n",
    "        axs[k].legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_median_time_serie(window=0.5*u.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# üîé **Data analysis**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¢ Noise sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sensitivity.plot(\n",
    "    scene_ideal=SCENE_IDEAL,\n",
    "    scene_perturbed=SCENE_PERTURBED,\n",
    "    scene_obs=SCENE_OBS,\n",
    "    scene_gen=SCENE_GEN,\n",
    "    limit=3*SHIFTS_CE_RMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òëÔ∏è Model fitting (WIP)\n",
    "\n",
    "Distributions are cool, but in order to make deeper analysis, we want to find a model that describe these distribution using few parameters. Unfortunately, there is no straightforward way to get such model as these distribution are very particular.\n",
    "\n",
    "The next block try most of the common distribution models and show the best ones... but unfortunately, none of them seems to match üòû"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    f = fitter.Fitter(data,\n",
    "           distributions=fitter.get_distributions())\n",
    "    f.fit()\n",
    "    f.summary()\n",
    "\n",
    "# fit(H0_IDEAL_INSTANT_SERIE['kernels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Test statistics\n",
    "\n",
    "A test statistic is a way of reducing the data we have to an unique number and compare this number to a threshold value. If the number is below the treshold, then the null hypothesis is favored. If it is above, the alternative hypothesis is favored. The goal is to find the best test statistic that allow to distinguish both hypothesis in a correct way\n",
    "\n",
    "- $H0$: the null hypothesis -> there is no planet\n",
    "- $H1$: the alternative hypothesis -> there is a planet\n",
    "\n",
    "- $T0$: vector of distributions obtained with H0\n",
    "- $T1$: vector of distributions obtained with H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T0, T1, TREF = ts.get_vectors(nmc=1000, size=100, scene_h1=SCENE_IDEAL.copy(input_ce_rms=100*u.nm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean\n",
    "\n",
    "**Principle:** We take the average of the distribution and we compare it to a treshold.\n",
    "\n",
    "$$\n",
    "\\left|\\frac{1}{N}\\sum_i x_i \\right| \\stackrel{H_1}{\\underset{H_0}{\\gtrless}} \\xi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "**Principle:** We take the median of the distribution and we compare it to a treshold.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\left| x_{\\frac{N+1}{2}} \\right| & \\text{if }N\\text{ is odd} \\\\\n",
    "\n",
    "\\left| \\frac{x_{\\frac{N}{2}} + x_{\\frac{N+1}{2}}}{2} \\right|  & \\text{if }N\\text{ is odd}\n",
    "\\end{cases}\n",
    "\\quad\\stackrel{H_1}{\\underset{H_0}{\\gtrless}} \\xi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax\n",
    "\n",
    "**Principle:** We pack our data in bins and we consider the position of the bin with the highest number of occurences. We compare it to a treshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov\n",
    "\n",
    "**Principle:** We compare the maximum distance on the cumulative distribution functions of the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cram√®r-von Mises\n",
    "\n",
    "**Principle:** We compare the total quadratique distance on the cumulative distribution functions of the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcoxon-Mann-Whitney (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDF diff area (Aur√©lie's idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_STATISTICS = {\n",
    "    'Mean': ts.mean,\n",
    "    'Median': ts.median,\n",
    "    # 'argmax50': ts.argmax50,\n",
    "    # 'argmax100': ts.argmax100,\n",
    "    'Argmax500': ts.argmax500,\n",
    "    'Kolmogorov-Smirnov': ts.kolmogorov_smirnov,\n",
    "    'Cramer Von Mises': ts.cramer_von_mises,\n",
    "    'Wilcoxon Mann Whitney': ts.wilcoxon_mann_whitney,\n",
    "    'CDF Diff. Area': ts.cdf_diff_area\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì ROC curves\n",
    "\n",
    "ROC curves allow to compare the power of different test statistics. It show the proportion of true detection in function of the probability of false alarm. The more the curve climb fast, the better it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.plot_roc(t0=T0, t1=T1, tref=TREF, test_statistics=TEST_STATISTICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ P-values\n",
    "\n",
    "The P-value is an indicator of the confidence we have to reject the null hypothesis.\n",
    "\n",
    "The principle consist in comparing the test statistic obtain on the data we want to test with a large bunch of data that we know to be under the null hypothesis. We then compute the proportion of test statistic that are above the one we obtained. This proportion is the P-value.\n",
    "\n",
    "Thus, the lower the P-value, the more confident we are to reject the null hypothesis. A P-value below 0.05 is commonly considered as a good indicator to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.plot_p_values(test_statistics=TEST_STATISTICS, t0=T0, tref=TREF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ü™ê **Characterization**\n",
    "\n",
    "</div>\n",
    "\n",
    "## üîÑÔ∏è Angular diversity\n",
    "\n",
    "In order to determine the planet position, we need to rotate the interferometer baseline in order to rotate the transmission map. Thus, the planet signal will be modulated. By analysis this modulation (trying to fit the parametrized modulation function to the data points), it is possible to retrieve the planet position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit()\n",
    "def kernels_modulation_njit(\n",
    "    h_range:np.ndarray[float]=H_RANGE.to(u.rad).value,\n",
    "    alpha:float=PARALLACTIC_ANGLE.to(u.rad).value,\n",
    "    theta:float=ANGULAR_SEPARATION.to(u.rad).value,\n",
    "    # contrast:float=CONTRAST\n",
    ") -> np.ndarray[float]:\n",
    "    \"\"\"\n",
    "    Compute the modulation of the kernels with respect to the hour angle\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - h_range: The range of hour angles to consider (in radian)\n",
    "    - alpha: Parallactic angle (in radian)\n",
    "    - theta: Angular separation (in radian)\n",
    "    - contrast: The contrast value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - The modulation of the kernels.\n",
    "    \"\"\"\n",
    "\n",
    "    kernels_mod = np.zeros((3, len(h_range)))\n",
    "    for h, dh in enumerate(h_range):\n",
    "\n",
    "        projected_telescope_position = project_position_njit(h=dh)\n",
    "\n",
    "        signals = get_input_fields_njit(\n",
    "            norm=PLANET_FLUX,\n",
    "            projected_telescope_positions=projected_telescope_position,\n",
    "            parallactic_angle=alpha,\n",
    "            angular_separation=theta,\n",
    "        )\n",
    "\n",
    "        _, kernels, _ = kn_njit(\n",
    "            signals,\n",
    "            shifts=np.zeros(14),\n",
    "            shifts_total_opd=np.zeros(14),\n",
    "        )\n",
    "\n",
    "        for i in range(3):\n",
    "            kernels_mod[i, h] = kernels[i] * CONTRAST * OPTICAL_EFFICIENCY\n",
    "\n",
    "    return kernels_mod\n",
    "\n",
    "def kernels_modulation(\n",
    "    h_range:u.Quantity=H_RANGE,\n",
    "    alpha:u.Quantity=PARALLACTIC_ANGLE,\n",
    "    theta:u.Quantity=ANGULAR_SEPARATION,\n",
    "    # contrast:float=CONTRAST\n",
    "):\n",
    "    return kernels_modulation_njit(h_range.to(u.rad).value, alpha.to(u.rad).value, theta.to(u.rad).value) # , contrast)\n",
    "\n",
    "kernel_modulation = [lambda h,a,t: kernels_modulation(h,a,t)[i] for i in range(3)]\n",
    "kernel_modulation_njit = [lambda h,a,t: kernels_modulation_njit(h,a,t)[i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iplot_kernels_modulation():\n",
    "    alpha_slider = widgets.FloatSlider(value=PARALLACTIC_ANGLE.value, min=0, max=2*np.pi, step=2*np.pi/1000, description=f'Alpha (rad)')\n",
    "    theta_slider = widgets.FloatSlider(value=ANGULAR_SEPARATION.value, min=0, max=FOV.value, step=FOV.value/1000, description=f'Theta ({FOV.unit})')\n",
    "    contrast_slider = widgets.FloatSlider(value=int(np.log10(CONTRAST)), min=-10, max=0, step=1, description=f'Contrast')\n",
    "    plot = widgets.Image()\n",
    "    reset = widgets.Button(description=\"Reset\")\n",
    "\n",
    "    def update_plot(*args):\n",
    "        kms = kernels_modulation(H_RANGE, alpha_slider.value*u.rad, (theta_slider.value*FOV.unit).to(u.rad)) # , 10**contrast_slider.value\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 3))\n",
    "\n",
    "        for i in range(3):\n",
    "            ax = axs[i]\n",
    "            ax.plot(H_RANGE, kms[i])\n",
    "            ax.set_title(f\"Kernel {i+1}\")\n",
    "            ax.set_xlabel(\"Hour angle (rad)\")\n",
    "            ax.set_ylabel(\"Intensity\")  \n",
    "\n",
    "        buffer = BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        plot.value = buffer.getvalue()\n",
    "        plt.close()\n",
    "\n",
    "    def reset_values(*args):\n",
    "        alpha_slider.value = PARALLACTIC_ANGLE.value\n",
    "        theta_slider.value = ANGULAR_SEPARATION.value\n",
    "        contrast_slider.value = int(np.log10(CONTRAST))\n",
    "\n",
    "    alpha_slider.observe(update_plot)\n",
    "    theta_slider.observe(update_plot)\n",
    "    contrast_slider.observe(update_plot)\n",
    "    reset.on_click(reset_values)\n",
    "\n",
    "    update_plot()\n",
    "    display(widgets.VBox([alpha_slider, theta_slider, contrast_slider, plot, reset]))\n",
    "\n",
    "iplot_kernels_modulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hour_diversity(\n",
    "        data=HOUR_DIVERSITY_DATA,\n",
    "        h_range=H_RANGE,\n",
    "    ):\n",
    "\n",
    "    fits = np.zeros((3, len(h_range)))\n",
    "\n",
    "    print(\"Param    = Ideal | First guess | Found\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    popts = []\n",
    "    for i in range(3):\n",
    "        # Fit data to kernels modulations\n",
    "        p0 = [PARALLACTIC_ANGLE.value + np.random.normal(0,0.1*np.pi), (ANGULAR_SEPARATION + np.random.normal(0,0.2)*u.mas).to(u.rad).value] # , CONTRAST * np.random.normal(1,0.1)\n",
    "\n",
    "        popt, pcov = curve_fit(\n",
    "            kernel_modulation_njit[i],\n",
    "            h_range.value,\n",
    "            data[i],\n",
    "            p0=p0,\n",
    "            maxfev=10000\n",
    "        )\n",
    "\n",
    "        print(f\"Alpha    = {PARALLACTIC_ANGLE.to(u.deg):.2f} | {(p0[0]*u.rad).to(u.deg):.2f} | {(popt[0]*u.rad).to(u.deg):.2f}\")\n",
    "        print(f\"Theta    = {ANGULAR_SEPARATION.to(u.mas):.2f} | {(p0[1]*u.rad).to(u.mas):.2f} | {(popt[1]*u.rad).to(u.mas):.2f}\")\n",
    "        # print(f\"Contrast = {CONTRAST:.2e} | {p0[2]:.2e} | {popt[2]:.2e}\")\n",
    "        print(\"---\")\n",
    "        popts.append(popt)\n",
    "    popt = np.mean(popts, axis=0)\n",
    "\n",
    "    _, axs = plt.subplots(3, 1, figsize=(15, 15))\n",
    "    for i in range(3):\n",
    "        fits[i] = kernels_modulation_njit(h_range.to(u.rad).value, *popt)[i]\n",
    "\n",
    "        ax = axs[i]\n",
    "\n",
    "        ax.scatter(h_range, data[i], label=\"Data\",alpha=0.5)\n",
    "        ax.plot(h_range, kernels_modulation_njit(h_range.to(u.rad).value)[i], \"--\", label=\"Expected signal\")\n",
    "        ax.plot(h_range, kernels_modulation_njit(h_range.to(u.rad).value, *popts[i])[i], label=\"Specific fit\")\n",
    "        ax.plot(h_range, fits[i], label=\"Global fit\")\n",
    "\n",
    "        ax.set_xlabel(\"Observation time (hour)\")\n",
    "        ax.set_ylabel(\"Kernel intensity\")\n",
    "        ax.set_title(f\"Kernel {i+1}\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Param    = Ideal | Found\")\n",
    "    print(\"========================\")\n",
    "    print(f\"Alpha    = {PARALLACTIC_ANGLE.to(u.deg):.2f} | {(popt[0]*u.rad).to(u.deg):.2f}\")\n",
    "    print(f\"Theta    = {ANGULAR_SEPARATION.to(u.mas):.2f} | {(popt[1]*u.rad).to(u.mas):.2f}\")\n",
    "    # print(f\"Contrast = {CONTRAST:.2e} | {popt[2]:.2e}\")\n",
    "\n",
    "plot_hour_diversity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåå On-sky contribution\n",
    "\n",
    "From the obtained data, it is possible to build a 2D distribution of the perceived sky contribution zones. This distribution provides insights into the possible locations of objects, enabling accurate initial estimations to fit the data points obtained based on the parallactic angle.\n",
    "\n",
    "This method involves stacking the transmission maps rotated by the baseline rotation and weighting each map by the corresponding data obtained for that baseline rotation.\n",
    "\n",
    "The base idea was already explored as \"image reconstruction\" technic using classical nulling interferometry $^1$. However, the method here is based on Kernel-Nulls which makes it more complex but less sensitive to phase aberations and by considerig the different Kernels, we can reduce the degeneracy of the solutions.\n",
    "\n",
    "Considering:\n",
    "- $T_{n}$ represents the n-th kernel's normalized transmission map.\n",
    "- $d_{n,\\beta}$ denotes the data point obtained for kernel $n$ with baseline rotation $\\beta$.\n",
    "- $\\alpha$ is the parallactic angle.\n",
    "- $\\theta$ is the angular separation.\n",
    "\n",
    "$$\n",
    "r_n(\\theta, \\alpha) = \\sum_a T_{n,h}(\\theta,\\alpha) d_{n,h}\n",
    "$$\n",
    "\n",
    "\n",
    "As the kernel outputs are antisymetric, we can filter the negative contributions:\n",
    "$$\n",
    "r'_n = \\frac{1}{2}\\max(r_n, 0)\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, we can compute the product over all the kernels to get the final contribution zones:\n",
    "$$\n",
    "C(\\theta, \\alpha) = \\prod_n r'_n(\\theta, \\alpha)\n",
    "$$\n",
    "\n",
    "**References:**\n",
    "1. Angel, J. R. P., et N. J. Woolf. \"An Imaging Nulling Interferometer to Study Extrasolar Planets\". *The Astrophysical Journal* 475, no 1 (1997): 373‚Äë79. https://doi.org/10.1086/303529."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contribution_zones(\n",
    "    data: np.ndarray[float] = HOUR_DIVERSITY_DATA,\n",
    "    fov: u.Quantity = FOV,\n",
    "    resolution: int = 100,\n",
    "):\n",
    "\n",
    "    images = np.zeros((3, resolution, resolution))\n",
    "    integrated_kernels = np.zeros((3, resolution, resolution))\n",
    "\n",
    "    _, _, theta_map, _ = get_uv_map(resolution=resolution, fov=fov)\n",
    "    _, _, kernel_maps = get_transmission_map(resolution=resolution)\n",
    "    theta_map = theta_map.value / np.max(theta_map.value)\n",
    "\n",
    "    for i, h in enumerate(H_RANGE):\n",
    "\n",
    "        for j in range(3):\n",
    "            projected_telescope_position = project_position(h=h)\n",
    "            tm = get_transmission_map(projected_telescope_positions=projected_telescope_position)[2][j]\n",
    "            integrated_kernels[j] += np.abs(tm) #* theta_map\n",
    "            images[j] += tm * data[j, i] #* theta_map\n",
    "\n",
    "    for i in range(3):\n",
    "        integrated_kernel_intensity = np.sum(integrated_kernels[i])\n",
    "        kernel_intensity = np.sum(np.abs(kernel_maps[i]))\n",
    "\n",
    "        normalization_factor = integrated_kernel_intensity / kernel_intensity\n",
    "\n",
    "        print(f\"Kernel {i+1} : {integrated_kernel_intensity:.2e} / {kernel_intensity:.2e} = {normalization_factor:.2e}\")\n",
    "\n",
    "        images[i] /= normalization_factor\n",
    "\n",
    "    max_im = np.max(images)\n",
    "\n",
    "    _, axs = plt.subplots(1, 4, figsize=(25, 5))\n",
    "\n",
    "    planet_x, planet_y = Œ±Œ∏_to_xy()\n",
    "\n",
    "    for i in range(3):\n",
    "        img = images[i]\n",
    "        img[img < 0] = 0\n",
    "        im = axs[i].imshow(img, cmap=\"hot\", vmax=max_im, extent=EXTENT)\n",
    "        axs[i].set_title(f\"Kernel {i+1}\")\n",
    "        plt.colorbar(im, ax=axs[i])\n",
    "        axs[i].scatter(planet_x, planet_y, marker=\"+\", color=\"white\")\n",
    "\n",
    "    # Take mean of probable input flux\n",
    "    img = np.mean(images, axis=0)\n",
    "\n",
    "    # Filter where a probability of flux is null on one of the kernels\n",
    "    images[images < 0] = 0\n",
    "    mask = np.prod(images, axis=0)\n",
    "    img = img * mask / np.max(mask)\n",
    "\n",
    "    # Plot reconstructed image\n",
    "    im = axs[3].imshow(img, cmap=\"hot\", extent=EXTENT)\n",
    "    axs[3].set_title(\"Contribution zones\")\n",
    "    plt.colorbar(im, ax=axs[3])\n",
    "    axs[3].scatter(planet_x, planet_y, marker=\"+\", color=\"white\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_contribution_zones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôí Correlation map (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_map(\n",
    "        data=HOUR_DIVERSITY_DATA,\n",
    "        h_range=H_RANGE,\n",
    "    ):\n",
    "\n",
    "    resolution = 20\n",
    "    \n",
    "    _, _, alpha_map, theta_map = get_uv_map(resolution=resolution)\n",
    "\n",
    "    correl_map = np.zeros((resolution, resolution))\n",
    "\n",
    "    for x in range(resolution):\n",
    "        for y in range(resolution):\n",
    "\n",
    "            alpha = alpha_map[x, y]\n",
    "            theta = theta_map[x, y]\n",
    "\n",
    "            km = kernels_modulation(h_range, alpha=alpha, theta=theta)[0]\n",
    "\n",
    "            correl_map[x, y] = np.sum(np.corrcoef(data[0], km))\n",
    "\n",
    "\n",
    "    _, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    im = ax.imshow(correl_map)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_title(\"Correlation map\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "correlation_map()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
